{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6756de5",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Import All Required Libraries\n",
    "Import necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: Import All Required Libraries\n",
    "# ============================================\n",
    "\n",
    "# Core Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Missing Data Visualization\n",
    "import missingno as msno\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Excel file support\n",
    "import openpyxl\n",
    "\n",
    "# ============================================\n",
    "# Configure Display Settings\n",
    "# ============================================\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Execution Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Pandas Version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09627ece",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: Load ALL Datasets From Dataset Folder\n",
    "Auto-detect and load all CSV and Excel files from the Dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: Load ALL Datasets From Dataset Folder\n",
    "# ============================================\n",
    "\n",
    "# Define the dataset folder path (relative to notebook location)\n",
    "DATASET_FOLDER = '../Dataset'\n",
    "\n",
    "# Auto-detect all CSV and Excel files\n",
    "csv_files = glob.glob(os.path.join(DATASET_FOLDER, '*.csv'))\n",
    "excel_files = glob.glob(os.path.join(DATASET_FOLDER, '*.xlsx'))\n",
    "\n",
    "all_files = csv_files + excel_files\n",
    "\n",
    "print(f\"üìÇ Dataset Folder: {os.path.abspath(DATASET_FOLDER)}\")\n",
    "print(f\"üìä Found {len(csv_files)} CSV files and {len(excel_files)} Excel files\")\n",
    "print(f\"üìÅ Total files to load: {len(all_files)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Load Each Dataset and Store in Dictionary\n",
    "# ============================================\n",
    "\n",
    "# Dictionary to store all datasets\n",
    "datasets = {}\n",
    "\n",
    "for file_path in all_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìÑ Loading: {filename}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Load based on file extension\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif filename.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        \n",
    "        # Create a clean key name for the dictionary\n",
    "        key_name = filename.replace('.csv', '').replace('.xlsx', '').replace(' ', '_').lower()\n",
    "        datasets[key_name] = df\n",
    "        \n",
    "        # Display dataset information\n",
    "        print(f\"\\nüìê Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"\\nüìã Column Names ({len(df.columns)} columns):\")\n",
    "        print(f\"   {list(df.columns)}\")\n",
    "        print(f\"\\nüîç Data Types:\")\n",
    "        print(df.dtypes.to_string())\n",
    "        print(f\"\\nüìä First 5 Rows:\")\n",
    "        display(df.head())\n",
    "        print(f\"\\n‚úÖ Successfully loaded: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {filename}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üì¶ SUMMARY: Loaded {len(datasets)} datasets successfully\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Dataset Overview Summary Table\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "for name, df in datasets.items():\n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': df.shape[0],\n",
    "        'Columns': df.shape[1],\n",
    "        'Missing Values': df.isnull().sum().sum(),\n",
    "        'Missing %': f\"{(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\",\n",
    "        'Memory (KB)': f\"{df.memory_usage(deep=True).sum() / 1024:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "# Total rows across all datasets\n",
    "total_rows = sum([df.shape[0] for df in datasets.values()])\n",
    "print(f\"\\nüìà Total rows across all datasets: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd803347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Display All Unique Columns Across Datasets\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìã ALL UNIQUE COLUMNS ACROSS ALL DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_columns = set()\n",
    "for name, df in datasets.items():\n",
    "    all_columns.update(df.columns.tolist())\n",
    "    print(f\"\\nüîπ {name}:\")\n",
    "    print(f\"   {list(df.columns)}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä Total Unique Columns Found: {len(all_columns)}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(sorted(all_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619dc1c",
   "metadata": {},
   "source": [
    "---\n",
    "## STEPS 3-13: Complete Machine Learning Pipeline\n",
    "\n",
    "The following cells contain the complete ML workflow from data merging to model deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: Create Master Column Schema (30 Features)\n",
    "# ============================================\n",
    "\n",
    "MASTER_SCHEMA = {\n",
    "    # Soil Features\n",
    "    'Nitrogen': 'float64',\n",
    "    'Phosphorus': 'float64',\n",
    "    'Potassium': 'float64',\n",
    "    'Soil_Quality': 'float64',\n",
    "    'Soil_Humidity': 'float64',\n",
    "    'Soil_Type': 'object',\n",
    "    'pH': 'float64',\n",
    "    \n",
    "    # Weather Features\n",
    "    'Rainfall_mm': 'float64',\n",
    "    'Temperature_C': 'float64',\n",
    "    'Humidity': 'float64',\n",
    "    'Sunshine_hours': 'float64',\n",
    "    'Air_Pressure_hPa': 'float64',\n",
    "    'Wind_Speed_kmph': 'float64',\n",
    "    'Moisture': 'float64',\n",
    "    \n",
    "    # Crop & Location Features\n",
    "    'Crop': 'object',\n",
    "    'State': 'object',\n",
    "    'Region': 'object',\n",
    "    'Season': 'object',\n",
    "    \n",
    "    # Agricultural Management Features\n",
    "    'Fertilizer_Amount_kg_per_hectare': 'float64',\n",
    "    'Irrigation_Schedule': 'object',\n",
    "    'Seed_Variety': 'object',\n",
    "    'Farm_Area_hectares': 'float64',\n",
    "    \n",
    "    # Economic Features\n",
    "    'Price_per_kg': 'float64',\n",
    "    'Production_Cost': 'float64',\n",
    "    \n",
    "    # Target Variable\n",
    "    'Yield_kg_per_hectare': 'float64',\n",
    "    \n",
    "    # Source tracking\n",
    "    'Source_Dataset': 'object',\n",
    "    'Year': 'int64',\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìã MASTER COLUMN SCHEMA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total Features Defined: {len(MASTER_SCHEMA)}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (col, dtype) in enumerate(MASTER_SCHEMA.items(), 1):\n",
    "    print(f\"{i:2}. {col:40} ‚Üí {dtype}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: Merge All Datasets Into Unified Dataset\n",
    "# ============================================\n",
    "\n",
    "# Column mapping dictionary (source ‚Üí master schema)\n",
    "COLUMN_MAPPING = {\n",
    "    # Soil Nutrients\n",
    "    'N_SOIL': 'Nitrogen', 'P_SOIL': 'Phosphorus', 'K_SOIL': 'Potassium',\n",
    "    'Nitrogen (N)': 'Nitrogen', 'Phosphorous (P)': 'Phosphorus', 'Pottasium (K)': 'Potassium',\n",
    "    \n",
    "    # Temperature\n",
    "    'TEMPERATURE': 'Temperature_C', 'Air temperature (C)': 'Temperature_C',\n",
    "    'Temperatue': 'Temperature_C', 'Mean Temp': 'Temperature_C',\n",
    "    \n",
    "    # Humidity\n",
    "    'HUMIDITY': 'Humidity', 'Air humidity (%)': 'Humidity', 'Average Humidity': 'Humidity',\n",
    "    \n",
    "    # Soil\n",
    "    'Soil humidity': 'Soil_Humidity', 'Soil Moisture (%)': 'Soil_Humidity',\n",
    "    'Soil_type': 'Soil_Type', 'Soil Type': 'Soil_Type',\n",
    "    \n",
    "    # Rainfall\n",
    "    'RAINFALL': 'Rainfall_mm', 'Mean Rainfall': 'Rainfall_mm', 'Average Rainfall': 'Rainfall_mm',\n",
    "    \n",
    "    # Yield (Target)\n",
    "    'Yield_kg_per_hectare': 'Yield_kg_per_hectare', 'Crop Yield': 'Yield_kg_per_hectare',\n",
    "    'Yeild (Q/acre)': 'Yield_kg_per_hectare', 'millet yield': 'Yield_kg_per_hectare',\n",
    "    \n",
    "    # Other mappings\n",
    "    'label': 'Crop', 'Crop_Type': 'Crop',\n",
    "    'STATE': 'State', 'State_Name': 'State',\n",
    "    'Fertilizer_Used_kg_per_hectare': 'Fertilizer_Amount_kg_per_hectare',\n",
    "    'Area (hect)': 'Farm_Area_hectares',\n",
    "    'Price': 'Price_per_kg',\n",
    "}\n",
    "\n",
    "def standardize_columns(df, source_name):\n",
    "    \"\"\"Rename columns to match master schema\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.rename(columns=COLUMN_MAPPING)\n",
    "    df_copy['Source_Dataset'] = source_name\n",
    "    return df_copy\n",
    "\n",
    "# Process each dataset\n",
    "standardized_dfs = []\n",
    "for name, df in datasets.items():\n",
    "    # Skip metadata files\n",
    "    if 'crop_data' in name.lower() and df.shape[0] < 20:\n",
    "        print(f\"‚è≠Ô∏è Skipping {name} (metadata only)\")\n",
    "        continue\n",
    "    \n",
    "    std_df = standardize_columns(df, name)\n",
    "    standardized_dfs.append(std_df)\n",
    "    print(f\"‚úÖ Standardized: {name} ({len(std_df)} rows)\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "unified_df = pd.concat(standardized_dfs, ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä UNIFIED DATASET CREATED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total Rows: {len(unified_df):,}\")\n",
    "print(f\"Total Columns: {len(unified_df.columns)}\")\n",
    "print(f\"\\nColumns: {list(unified_df.columns)}\")\n",
    "\n",
    "# Save unified dataset\n",
    "unified_df.to_csv('unified_dataset.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved: unified_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ed75a",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5-7: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: Data Quality Assessment\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nüìê Dataset Shape: {unified_df.shape[0]:,} rows √ó {unified_df.shape[1]} columns\")\n",
    "\n",
    "# Missing values analysis\n",
    "print(f\"\\nüìã MISSING VALUES ANALYSIS:\")\n",
    "print(\"-\" * 70)\n",
    "missing = unified_df.isnull().sum()\n",
    "missing_pct = (missing / len(unified_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "print(missing_df)\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nüî¢ DATA TYPES:\")\n",
    "print(\"-\" * 70)\n",
    "print(unified_df.dtypes)\n",
    "\n",
    "# Statistical summary for numeric columns\n",
    "print(f\"\\nüìà STATISTICAL SUMMARY (Numeric Columns):\")\n",
    "print(\"-\" * 70)\n",
    "display(unified_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc903aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6: EDA Visualizations\n",
    "# ============================================\n",
    "\n",
    "# Create plots directory\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Filter rows with yield data for visualization\n",
    "yield_df = unified_df.dropna(subset=['Yield_kg_per_hectare'])\n",
    "print(f\"üìä Rows with yield data: {len(yield_df):,}\")\n",
    "\n",
    "# Plot 1: Yield Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(yield_df['Yield_kg_per_hectare'], bins=50, color='#4CAF50', edgecolor='white', alpha=0.8)\n",
    "axes[0].set_xlabel('Yield (kg/hectare)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Crop Yield', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(yield_df['Yield_kg_per_hectare'].mean(), color='red', linestyle='--', label=f'Mean: {yield_df[\"Yield_kg_per_hectare\"].mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(yield_df['Yield_kg_per_hectare'], vert=True)\n",
    "axes[1].set_ylabel('Yield (kg/hectare)', fontsize=12)\n",
    "axes[1].set_title('Yield Box Plot', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/yield_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: plots/yield_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ea594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Correlation Matrix\n",
    "numeric_cols = unified_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) > 1:\n",
    "    corr_matrix = unified_df[numeric_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                center=0, square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Saved: plots/correlation_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Crop-wise Yield Comparison (if Crop column exists)\n",
    "if 'Crop' in yield_df.columns and yield_df['Crop'].notna().sum() > 10:\n",
    "    crop_yield = yield_df.groupby('Crop')['Yield_kg_per_hectare'].agg(['mean', 'count']).reset_index()\n",
    "    crop_yield = crop_yield[crop_yield['count'] >= 5].nlargest(15, 'mean')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.barh(crop_yield['Crop'], crop_yield['mean'], color=plt.cm.Greens(np.linspace(0.3, 0.9, len(crop_yield))))\n",
    "    plt.xlabel('Average Yield (kg/hectare)', fontsize=12)\n",
    "    plt.ylabel('Crop Type', fontsize=12)\n",
    "    plt.title('Top 15 Crops by Average Yield', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    for bar, val in zip(bars, crop_yield['mean']):\n",
    "        plt.text(val + 10, bar.get_y() + bar.get_height()/2, f'{val:.0f}', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/crop_yield_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Saved: plots/crop_yield_comparison.png\")\n",
    "\n",
    "# Plot 4: State-wise yield (if State column exists)\n",
    "if 'State' in yield_df.columns and yield_df['State'].notna().sum() > 10:\n",
    "    state_yield = yield_df.groupby('State')['Yield_kg_per_hectare'].mean().nlargest(10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    state_yield.plot(kind='bar', color='#2196F3', edgecolor='white')\n",
    "    plt.xlabel('State', fontsize=12)\n",
    "    plt.ylabel('Average Yield (kg/hectare)', fontsize=12)\n",
    "    plt.title('Top 10 States by Average Crop Yield', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/state_yield_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Saved: plots/state_yield_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788dbbc",
   "metadata": {},
   "source": [
    "---\n",
    "## STEPS 8-10: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8: Data Preparation for Modeling\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ü§ñ DATA PREPARATION FOR MODELING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter data with target variable\n",
    "model_df = unified_df.dropna(subset=['Yield_kg_per_hectare']).copy()\n",
    "print(f\"Rows with target variable: {len(model_df):,}\")\n",
    "\n",
    "# Select features for modeling\n",
    "FEATURE_COLS = [\n",
    "    'Rainfall_mm', 'Temperature_C', 'Humidity', 'Soil_Quality',\n",
    "    'Nitrogen', 'Phosphorus', 'Potassium',\n",
    "    'Fertilizer_Amount_kg_per_hectare', 'Sunshine_hours', 'Soil_Humidity',\n",
    "    'Irrigation_Schedule', 'Seed_Variety'\n",
    "]\n",
    "\n",
    "# Filter to features that exist in our data\n",
    "available_features = [col for col in FEATURE_COLS if col in model_df.columns]\n",
    "print(f\"\\nüìã Available Features for Modeling ({len(available_features)}):\")\n",
    "for i, col in enumerate(available_features, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = model_df[available_features].copy()\n",
    "y = model_df['Yield_kg_per_hectare'].copy()\n",
    "\n",
    "# Handle categorical columns with Label Encoding\n",
    "label_encoders = {}\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"‚úÖ Encoded: {col} ({len(le.classes_)} categories)\")\n",
    "\n",
    "print(f\"\\nüìê Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"üìê Target Vector Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 9: Preprocessing & Train-Test Split\n",
    "# ============================================\n",
    "\n",
    "# Handle missing values with median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üîÑ Train-Test Split Complete:\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Testing samples: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ff16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 10: Train & Compare Multiple Models\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ü§ñ MODEL TRAINING & COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "best_model = None\n",
    "best_r2 = -float('inf')\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training: {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'R¬≤ Score': r2,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse\n",
    "    })\n",
    "    \n",
    "    print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"   MAE: {mae:.2f}\")\n",
    "    print(f\"   RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "# Display comparison table\n",
    "results_df = pd.DataFrame(results).sort_values('R¬≤ Score', ascending=False)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìä MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   R¬≤ Score: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e856f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Feature Importance Visualization\n",
    "# ============================================\n",
    "\n",
    "# Get feature importance from Random Forest\n",
    "rf_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': available_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(feature_importance['Feature'], feature_importance['Importance'], \n",
    "                color=plt.cm.Greens(np.linspace(0.3, 0.9, len(feature_importance))))\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: plots/feature_importance.png\")\n",
    "\n",
    "print(\"\\nüìä Feature Importance Ranking:\")\n",
    "for i, (_, row) in enumerate(feature_importance.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Feature']}: {row['Importance']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002825fd",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 11: Save Model & Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 11: Save Model and Preprocessing Objects\n",
    "# ============================================\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Save all artifacts\n",
    "artifacts = {\n",
    "    'model/model.pkl': best_model,\n",
    "    'model/scaler.pkl': scaler,\n",
    "    'model/imputer.pkl': imputer,\n",
    "    'model/label_encoders.pkl': label_encoders,\n",
    "    'model/feature_list.pkl': available_features,\n",
    "}\n",
    "\n",
    "# Model metadata\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'r2_score': best_r2,\n",
    "    'mae': results_df[results_df['Model'] == best_model_name]['MAE'].values[0],\n",
    "    'rmse': results_df[results_df['Model'] == best_model_name]['RMSE'].values[0],\n",
    "    'features': available_features,\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "artifacts['model/model_info.pkl'] = model_info\n",
    "\n",
    "# Save all artifacts\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ SAVING MODEL ARTIFACTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for path, obj in artifacts.items():\n",
    "    joblib.dump(obj, path)\n",
    "    print(f\"‚úÖ Saved: {path}\")\n",
    "\n",
    "print(f\"\\nüéâ Model saved successfully!\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "print(f\"   R¬≤ Score: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b7a57",
   "metadata": {},
   "source": [
    "---\n",
    "## STEPS 12-13: API & Dashboard (Pre-built)\n",
    "\n",
    "The Flask REST API and Web Dashboard have been created in the following locations:\n",
    "\n",
    "**API Files:**\n",
    "- `api/app.py` - Flask REST API with endpoints for predictions\n",
    "- `api/requirements.txt` - Python dependencies\n",
    "\n",
    "**Dashboard Files:**\n",
    "- `dashboard/index.html` - User-facing prediction form\n",
    "- `dashboard/technical.html` - Technical documentation for data scientists\n",
    "- `dashboard/style.css` - Modern responsive styles\n",
    "- `dashboard/script.js` - Frontend JavaScript\n",
    "\n",
    "**To Run the API:**\n",
    "```bash\n",
    "cd Phase-2/api\n",
    "python app.py\n",
    "# API runs on http://localhost:5000\n",
    "```\n",
    "\n",
    "**API Endpoints:**\n",
    "- `GET /health` - Health check\n",
    "- `GET /features` - List input features\n",
    "- `GET /model-info` - Model metadata\n",
    "- `POST /predict` - Single prediction\n",
    "- `POST /predict-batch` - Batch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 13: Test Prediction Function\n",
    "# ============================================\n",
    "\n",
    "def predict_yield(input_features):\n",
    "    \"\"\"\n",
    "    Make a yield prediction using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        input_features: dict with feature values\n",
    "        \n",
    "    Returns:\n",
    "        Predicted yield in kg/hectare\n",
    "    \"\"\"\n",
    "    # Load model artifacts\n",
    "    model = joblib.load('model/model.pkl')\n",
    "    scaler = joblib.load('model/scaler.pkl')\n",
    "    imputer = joblib.load('model/imputer.pkl')\n",
    "    features = joblib.load('model/feature_list.pkl')\n",
    "    \n",
    "    # Build feature vector\n",
    "    X = np.array([[input_features.get(f, np.nan) for f in features]])\n",
    "    \n",
    "    # Preprocess\n",
    "    X_imputed = imputer.transform(X)\n",
    "    X_scaled = scaler.transform(X_imputed)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(X_scaled)[0]\n",
    "    return max(0, prediction)\n",
    "\n",
    "# Test prediction\n",
    "test_input = {\n",
    "    'Rainfall_mm': 500,\n",
    "    'Temperature_C': 28,\n",
    "    'Humidity': 75,\n",
    "    'Soil_Quality': 80,\n",
    "    'Nitrogen': 45,\n",
    "    'Phosphorus': 50,\n",
    "    'Potassium': 40,\n",
    "    'Fertilizer_Amount_kg_per_hectare': 150,\n",
    "    'Sunshine_hours': 100,\n",
    "    'Soil_Humidity': 60,\n",
    "    'Irrigation_Schedule': 5,\n",
    "    'Seed_Variety': 1\n",
    "}\n",
    "\n",
    "predicted_yield = predict_yield(test_input)\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ TEST PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nInput Features:\")\n",
    "for k, v in test_input.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "print(f\"\\nüåæ Predicted Yield: {predicted_yield:.2f} kg/hectare\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc61347",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ WORKFLOW COMPLETE\n",
    "\n",
    "### Summary:\n",
    "- **Total Records Processed:** 7,109\n",
    "- **Features Used:** 12\n",
    "- **Best Model:** Gradient Boosting Regressor\n",
    "- **R¬≤ Score:** 0.9750 (97.5% variance explained)\n",
    "- **MAE:** 37.72 kg/hectare\n",
    "- **RMSE:** 52.20 kg/hectare\n",
    "\n",
    "### Files Created:\n",
    "1. `unified_dataset.csv` - Merged dataset\n",
    "2. `model/*.pkl` - Trained model and preprocessing objects\n",
    "3. `plots/*.png` - EDA visualizations\n",
    "4. `api/app.py` - Flask REST API\n",
    "5. `dashboard/*.html` - Web dashboard with technical docs\n",
    "\n",
    "### Next Steps:\n",
    "1. Run the API: `python api/app.py`\n",
    "2. Open `dashboard/index.html` in browser\n",
    "3. View technical documentation: `dashboard/technical.html`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
